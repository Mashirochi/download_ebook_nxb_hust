import os
import requests
from PIL import Image
from bs4 import BeautifulSoup
import re
import unicodedata
import sys
import time
from variable import book_url, GA, VISITOR, ANTIFORGERY, AUTH, GA_TRACK
import random

### C·∫§U H√åNH CH∆Ø∆†NG TR√åNH ###
# C·∫•u h√¨nh chu·∫©n c·ªßa t·ªõ n·∫øu m·ªçi ng∆∞·ªùi th√≠ch thay ƒë·ªïi g√¨ t·ª± ƒë·ªïi True/False nh√©
DOWNLOAD_IMAGE = True  # üî• ƒë·ªïi th√†nh False n·∫øu b·∫°n ƒë√£ c√≥ ·∫£nh v√† ch·ªâ mu·ªën t·∫°o PDF
SHOULD_MERGE_TO_PDF = True  # üî• b·∫≠t/t·∫Øt g·ªôp PDF (merge image to pdf)
DELETE_FOLDER_AFTER_MERGE = True  # üî• b·∫≠t/t·∫Øt x√≥a folder sau khi merge PDF xong
ENABLE_FAILED_IMAGE = False  # üî• True/False/None: False = skip merge n·∫øu c√≥ 5 ·∫£nh l·ªói li√™n ti·∫øp, True/None = ch·∫°y h·∫øt r·ªìi merge
DEBUG_PROGRAM = False  # üî• True ƒë·ªÉ gi·ªØ l·∫°i c√°c file t·∫°m, False ƒë·ªÉ x√≥a
MAKE_COLOR = True  # üî• L√†m m√†u
CONVERT_FILE_NAMES_TO_SLUG = True  # üî• True ƒë·ªÉ chuy·ªÉn t√™n file PDF th√†nh d·∫°ng slug
X_NO_RETRY = 2  # üî• S·ªë l·∫ßn retry khi l·∫•y tham s·ªë t·ª´ ebook link (th·ª≠ /0/1 tr∆∞·ªõc, n·∫øu l·ªói th√¨ th·ª≠ link g·ªëc)
#
###

### THAM S·ªê N·∫æU L·ªñI KHI CH·∫†Y CH∆Ø∆†NG TR√åNH ###
# T√™n pdf sau khi merge th√†nh c√¥ng (ƒë·ªÉ tr·ªëng ho·∫∑c "result.pdf" ƒë·ªÉ t·ª± ƒë·ªông l·∫•y t·ª´ title)
output_pdf = "result.pdf"
manual_ebook_link = ""
total_pages = 0
# C√°ch l·∫•y base_url
# L√†m b∆∞·ªõc 0 nh∆∞ ·∫£nh buoc_0.png r·ªìi xem video
base_url = ""
id_sach = ""
# Th∆∞ m·ª•c l∆∞u t·∫°m c√°c image b·∫°n t·∫£i
save_dir = "img"
###

### T·ª´ ƒëo·∫°n d∆∞·ªõi tr·ªü ƒëi n·∫øu kh√¥ng r√†nh code th√¨ ƒë·ª´ng s·ª≠a g√¨ nh√© ###
# ===== ANSI COLORS =====
RED = "\033[31m"
GREEN = "\033[32m"
YELLOW = "\033[33m"
BLUE = "\033[34m"
CYAN = "\033[36m"
MAGENTA = "\033[35m"
RESET = "\033[0m"


def animated_print(text, color="", duration=5):
    """Print text with a crypto-style animation, showing characters one by one"""
    if not MAKE_COLOR:
        print(f"{color}{text}{RESET}")
        return

    chars = list(text)
    if len(chars) > 0:
        delay = duration / len(chars)

        for char in chars:
            sys.stdout.write(f"{color}{char}{RESET}")
            sys.stdout.flush()
            time.sleep(delay)

        print()


def cleanup_temp_files():
    if not DEBUG_PROGRAM:
        temp_files = ["raw.html", "temp.js"]
        for temp_file in temp_files:
            if os.path.exists(temp_file):
                os.remove(temp_file)
                if DEBUG_PROGRAM:
                    print(f"{MAGENTA}üóë ƒê√£ x√≥a file t·∫°m: {temp_file}{RESET}")


def build_cookies():
    cookies = {}
    if GA:
        cookies["_ga"] = GA
    if VISITOR:
        cookies["visitorId"] = VISITOR
    if ANTIFORGERY:
        cookies[".AspNetCore.Antiforgery.PAnxZgrQbk8"] = ANTIFORGERY
    if AUTH:
        cookies["auth"] = AUTH
    if GA_TRACK:
        cookies["_ga_HFDYKEJJ3N"] = GA_TRACK
    return cookies


def validate_cookies():
    if not GA and not VISITOR and not ANTIFORGERY and not AUTH and not GA_TRACK:
        print(f"{RED}‚ùå L·ªói: Vui l√≤ng ƒëi·ªÅn cookie{RESET}")
        print(f"{YELLOW}üí° H∆∞·ªõng d·∫´n: Control shift I => Application => Cookies => https://nxbbachkhoa.vn/{RESET}")
        exit(1)


def get_ebook_info_from_book_url(book_url):
    if DEBUG_PROGRAM:
        print(f"{YELLOW}üìñ ƒêang l·∫•y th√¥ng tin t·ª´ URL s√°ch...{RESET}")

    response = requests.get(book_url)
    response.encoding = response.apparent_encoding
    soup = BeautifulSoup(response.text, "html.parser")
    with open("raw.html", "w", encoding=response.encoding) as f:
        f.write(soup.prettify())

    if not DEBUG_PROGRAM and os.path.exists("raw.html"):
        os.remove("raw.html")

    ebook_info = {}

    ebook_section = soup.find("div", class_="BookDetailSection-actions")
    if ebook_section:
        ebook_links = ebook_section.find_all("a", href=re.compile(r"/ebook/\d+"))

        short_link = None
        long_link = None

        for link in ebook_links:
            href = link["href"]
            if re.match(r"/ebook/\d+/.+", href):
                long_link = href
            else:
                short_link = href

        selected_ebook_link = long_link if long_link else short_link

        if selected_ebook_link:
            if selected_ebook_link.startswith("/"):
                selected_ebook_link = "https://nxbbachkhoa.vn" + selected_ebook_link

            ebook_info["link_ebook"] = selected_ebook_link

    page_count = None

    details_section = soup.find("div", class_="details-table-list")
    if details_section:
        page_elements = details_section.find_all(string=re.compile(r"S·ªë trang"))
        for element in page_elements:
            parent = element.parent
            if parent:
                next_elements = parent.find_next_siblings()
                for next_el in next_elements:
                    text = next_el.get_text(strip=True)
                    if text.isdigit():
                        page_count = text
                        break
            if page_count:
                break

    if not page_count:
        html_text = str(soup)
        page_match = re.search(r"S·ªë trang[^>]*>[\s]*<td[^>]*>[\s]*(\d+)", html_text)
        if page_match:
            page_count = page_match.group(1)

    if not page_count:
        all_text = soup.get_text()
        page_match = re.search(r"S·ªë trang[^\d]*(\d+)", all_text)
        if page_match:
            page_count = page_match.group(1)

    ebook_info["page_count"] = page_count

    title_tag = soup.find("title")
    if title_tag:
        title = title_tag.get_text(strip=True)
        ebook_info["title"] = title

    return ebook_info


def get_id_sach_from_ebook_link(ebook_link, cookies):
    if DEBUG_PROGRAM:
        print(f"{YELLOW}üîç ƒêang l·∫•y id_sach t·ª´ link ebook...{RESET}")

    base_ebook_link = re.sub(r"(/ebook/\d+)(/.*)?$", r"\1", ebook_link)
    link_with_suffix = base_ebook_link + "/0/1"
    link_without_suffix = base_ebook_link
    last_error = None

    for attempt in range(X_NO_RETRY):
        for link in [link_with_suffix, link_without_suffix]:
            try:
                response = requests.get(link, cookies=cookies, timeout=10)

                if response.status_code != 200:
                    if link == link_with_suffix:
                        if DEBUG_PROGRAM:
                            print(f"{YELLOW}‚ö†Ô∏è  Link v·ªõi /0/1 l·ªói (Status {response.status_code}), th·ª≠ link g·ªëc...{RESET}")
                        continue
                    else:
                        continue

                soup = BeautifulSoup(response.text, "html.parser")
                script_tag = soup.find("script", src=re.compile(r"mobile/javascript/config\.js"))

                if script_tag:
                    script_url = script_tag["src"]

                    if script_url.startswith("//"):
                        script_url = "https:" + script_url
                    elif script_url.startswith("/"):
                        script_url = "https://nxbbachkhoa.vn" + script_url

                    script_response = requests.get(script_url, cookies=cookies)
                    content = script_response.text

                    match = re.search(r'bookConfig\.CreatedTime\s*=\s*"?(\d+)"?', content)

                    if match:
                        if link == link_without_suffix:
                            print(f"{GREEN}‚úì T√¨m th·∫•y id_sach t·ª´ link g·ªëc (kh√¥ng c√≥ /0/1): {link}{RESET}")
                        return match.group(1)
                else:
                    if link == link_with_suffix:
                        if DEBUG_PROGRAM:
                            print(f"{YELLOW}‚ö†Ô∏è  Link v·ªõi /0/1 kh√¥ng c√≥ script config, th·ª≠ link g·ªëc...{RESET}")
                        continue
            except Exception as e:
                last_error = e
                if link == link_with_suffix:
                    if DEBUG_PROGRAM:
                        print(f"{YELLOW}‚ö†Ô∏è  Link v·ªõi /0/1 l·ªói: {e}, th·ª≠ link g·ªëc...{RESET}")
                    continue
                else:
                    break

        if attempt < X_NO_RETRY - 1:
            if DEBUG_PROGRAM:
                print(f"{YELLOW}‚ö†Ô∏è  Retry l·∫ßn {attempt + 2}/{X_NO_RETRY}...{RESET}")
            time.sleep(1)

    if last_error:
        error_msg = f"Kh√¥ng th·ªÉ l·∫•y id_sach t·ª´ ebook link sau {X_NO_RETRY} l·∫ßn th·ª≠. L·ªói cu·ªëi: {last_error}"
        print(f"{RED}‚ùå {error_msg}{RESET}")
        raise Exception(error_msg)

    return None


def get_base_url_from_ebook_link(ebook_link, cookies):
    """L·∫•y base_url t·ª´ link ebook, t·ª± ƒë·ªông th·ª≠ /0/1 tr∆∞·ªõc, n·∫øu l·ªói th√¨ retry v·ªõi link g·ªëc"""
    if DEBUG_PROGRAM:
        print(f"{YELLOW}üîó ƒêang l·∫•y base_url t·ª´ link ebook...{RESET}")

    headers = {"User-Agent": "Mozilla/5.0"}

    base_ebook_link = re.sub(r"(/ebook/\d+)(/.*)?$", r"\1", ebook_link)
    link_with_suffix = base_ebook_link + "/0/1"
    link_without_suffix = base_ebook_link

    last_error = None

    for attempt in range(X_NO_RETRY):
        for link in [link_with_suffix, link_without_suffix]:
            try:
                response = requests.get(link, headers=headers, cookies=cookies, timeout=10)

                if response.status_code != 200:
                    if link == link_with_suffix:
                        if DEBUG_PROGRAM:
                            print(f"{YELLOW}‚ö†Ô∏è  Link v·ªõi /0/1 l·ªói (Status {response.status_code}), th·ª≠ link g·ªëc...{RESET}")
                        continue
                    else:
                        continue

                html = response.text
                urls = re.findall(r'https?://[^\s"\'<>]+', html)

                mobile_urls = {u for u in urls if "/files/mobile" in u}

                if mobile_urls:
                    if link == link_without_suffix:
                        print(f"{GREEN}‚úì T√¨m th·∫•y base_url t·ª´ link g·ªëc (kh√¥ng c√≥ /0/1): {link}{RESET}")
                    return list(mobile_urls)[0]
                else:
                    if link == link_with_suffix:
                        if DEBUG_PROGRAM:
                            print(f"{YELLOW}‚ö†Ô∏è  Link v·ªõi /0/1 kh√¥ng c√≥ mobile URL, th·ª≠ link g·ªëc...{RESET}")
                        continue
            except Exception as e:
                last_error = e
                if link == link_with_suffix:
                    if DEBUG_PROGRAM:
                        print(f"{YELLOW}‚ö†Ô∏è  Link v·ªõi /0/1 l·ªói: {e}, th·ª≠ link g·ªëc...{RESET}")
                    continue
                else:
                    break

        if attempt < X_NO_RETRY - 1:
            if DEBUG_PROGRAM:
                print(f"{YELLOW}‚ö†Ô∏è  Retry l·∫ßn {attempt + 2}/{X_NO_RETRY}...{RESET}")
            time.sleep(1)
    if last_error:
        error_msg = f"Kh√¥ng th·ªÉ l·∫•y base_url t·ª´ ebook link sau {X_NO_RETRY} l·∫ßn th·ª≠. L·ªói cu·ªëi: {last_error}"
        print(f"{RED}‚ùå {error_msg}{RESET}")
        raise Exception(error_msg)

    return None


def crypto_print(text, color=CYAN, speed=0.02, noise_level=10):
    charset = "ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789!@#$%&*?"

    for final_char in text:
        current_char = random.choice(charset)

        for _ in range(noise_level):
            sys.stdout.write(f"{color}{current_char}{RESET}")
            sys.stdout.flush()
            time.sleep(speed)
            sys.stdout.write("\b")
            current_char = random.choice(charset)

        sys.stdout.write(f"{color}{final_char}{RESET}")
        sys.stdout.flush()
        time.sleep(speed)

    print()


def auto_fill_parameters():
    """T·ª± ƒë·ªông ƒëi·ªÅn c√°c tham s·ªë n·∫øu ch√∫ng r·ªóng"""
    global total_pages, base_url, id_sach, book_url, output_pdf

    validate_cookies()
    cookies = build_cookies()

    need_info = not total_pages or total_pages == 0 or not base_url or not id_sach or not book_url

    if not need_info:
        if DEBUG_PROGRAM:
            print(f"{GREEN}‚úì T·∫•t c·∫£ tham s·ªë ƒë√£ ƒë∆∞·ª£c ƒëi·ªÅn, b·ªè qua b∆∞·ªõc t·ª± ƒë·ªông l·∫•y th√¥ng tin{RESET}")
        return

    if not book_url:
        print(f"{RED}‚ùå Vui l√≤ng ƒëi·ªÅn book_url ƒë·ªÉ t·ª± ƒë·ªông l·∫•y th√¥ng tin{RESET}")
        return

    if DEBUG_PROGRAM:
        print(f"{CYAN}üîÑ B·∫Øt ƒë·∫ßu t·ª± ƒë·ªông l·∫•y th√¥ng tin...{RESET}\n")

    ebook_info = get_ebook_info_from_book_url(book_url)
    ebook_link = ebook_info.get("link_ebook")

    if not total_pages and ebook_info.get("page_count"):
        total_pages = int(ebook_info["page_count"])
        if DEBUG_PROGRAM:
            print(f"{GREEN}‚úì S·ªë trang: {total_pages}{RESET}")

    if not ebook_link:
        if ebook_info.get("page_count"):
            if manual_ebook_link:
                ebook_link = manual_ebook_link
                if DEBUG_PROGRAM:
                    print(f"{YELLOW}‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y link ebook t·ª± ƒë·ªông, s·ª≠ d·ª•ng link th·ªß c√¥ng{RESET}")
                    print(f"{GREEN}‚úì Link ebook (th·ªß c√¥ng): {ebook_link}{RESET}")
            else:
                if DEBUG_PROGRAM:
                    print(f"{YELLOW}‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y link ebook t·ª± ƒë·ªông{RESET}")
                    print(f"{YELLOW}üí° B·∫°n c√≥ th·ªÉ ƒëi·ªÅn manual_ebook_link n·∫øu c√≥ s·ªë trang{RESET}")
                if base_url and id_sach:
                    if DEBUG_PROGRAM:
                        print(f"{GREEN}‚úì ƒê√£ c√≥ base_url v√† id_sach th·ªß c√¥ng, ti·∫øp t·ª•c...{RESET}")
                else:
                    print(f"{RED}‚ùå C·∫ßn link ebook ƒë·ªÉ l·∫•y base_url v√† id_sach{RESET}")
                    return
        else:
            print(f"{RED}‚ùå Kh√¥ng t√¨m th·∫•y link ebook v√† s·ªë trang t·ª´ URL s√°ch{RESET}")
            return
    else:
        if DEBUG_PROGRAM:
            print(f"{GREEN}‚úì Link ebook: {ebook_link}{RESET}")

    if not id_sach and ebook_link:
        id_sach = get_id_sach_from_ebook_link(ebook_link, cookies)
        if id_sach:
            if DEBUG_PROGRAM:
                print(f"{GREEN}‚úì id_sach: {id_sach}{RESET}")
        else:
            print(f"{RED}‚ùå Kh√¥ng t√¨m th·∫•y id_sach{RESET}")

    if not base_url and ebook_link:
        base_url = get_base_url_from_ebook_link(ebook_link, cookies)
        if base_url:
            if DEBUG_PROGRAM:
                print(f"{GREEN}‚úì base_url: {base_url}{RESET}")
        else:
            print(f"{RED}‚ùå Kh√¥ng t√¨m th·∫•y base_url{RESET}")

    if not output_pdf or output_pdf.strip() == "" or output_pdf == "result.pdf":
        title = ebook_info.get("title")
        if title:
            if CONVERT_FILE_NAMES_TO_SLUG:
                pdf_name = slugify(title)
            else:
                pdf_name = re.sub(r'[<>:"/\\|?*]', "", title).strip()

                MAX_FILE_NAME = 200
                if len(pdf_name) > MAX_FILE_NAME:
                    pdf_name = pdf_name[:MAX_FILE_NAME]

            if pdf_name:
                output_pdf = f"{pdf_name}.pdf"
                if DEBUG_PROGRAM:
                    print(f"{GREEN}‚úì T√™n file PDF: {output_pdf}{RESET}")
            else:
                output_pdf = "result.pdf"
                if DEBUG_PROGRAM:
                    print(f"{YELLOW}‚ö†Ô∏è  Kh√¥ng t·∫°o ƒë∆∞·ª£c t√™n file t·ª´ title, s·ª≠ d·ª•ng: {output_pdf}{RESET}")
        else:
            output_pdf = "result.pdf"
            if DEBUG_PROGRAM:
                print(f"{YELLOW}‚ö†Ô∏è  Kh√¥ng l·∫•y ƒë∆∞·ª£c title, s·ª≠ d·ª•ng t√™n m·∫∑c ƒë·ªãnh: {output_pdf}{RESET}")

    if DEBUG_PROGRAM:
        print(f"\n{CYAN}‚úÖ Ho√†n t·∫•t t·ª± ƒë·ªông l·∫•y th√¥ng tin{RESET}\n")


def slugify(text: str) -> str:
    text = unicodedata.normalize("NFKD", text).encode("ascii", "ignore").decode("utf-8")
    text = text.lower()
    text = re.sub(r"[^a-z0-9]+", "-", text)
    text = text.strip("-")
    return text


def main():
    validate_cookies()
    auto_fill_parameters()

    if not total_pages or total_pages == 0:
        print(f"{RED}‚ùå L·ªói: total_pages ch∆∞a ƒë∆∞·ª£c ƒëi·ªÅn{RESET}")
        exit(1)

    if not base_url:
        print(f"{RED}‚ùå L·ªói: base_url ch∆∞a ƒë∆∞·ª£c ƒëi·ªÅn{RESET}")
        exit(1)

    if not id_sach:
        print(f"{RED}‚ùå L·ªói: id_sach ch∆∞a ƒë∆∞·ª£c ƒëi·ªÅn{RESET}")
        exit(1)

    cookie_header = "; ".join([GA, VISITOR, ANTIFORGERY, AUTH, GA_TRACK])

    headers = {
        "User-Agent": "Mozilla/5.0",
        "Referer": "https://nxbbachkhoa.vn/",
        "Cookie": cookie_header,
    }

    print(f"{RED}C·∫£m ∆°n qu√Ω kh√°ch ƒë√£ s·ª≠ d·ª•ng d·ªãch v·ª• t·∫£i xu·ªëng mi·ªÖn ph√≠ kh√¥ng c·ªßa ƒê·∫°i h·ªçc B√°ch Khoa H√† N·ªôi.{RESET}")
    print(f"{RED}Qu√Ω kh√°ch vui l√≤ng ch·ªâ s·ª≠ d·ª•ng d·ªãch v·ª• cho m·ª•c ƒë√≠ch h·ªçc t·∫≠p v√† nghi√™n c·ª©u.{RESET}")
    print(f"{RED}M·ªçi h√†nh vi s·ª≠ d·ª•ng v·ªõi m·ª•c ƒë√≠ch th∆∞∆°ng m·∫°i ƒë·ªÅu b·ªã NGHI√äM C·∫§M.{RESET}")
    print(f"{RED}T√°c gi·∫£ v√† ƒë∆°n v·ªã tri·ªÉn khai tuy√™n b·ªë mi·ªÖn tr·ª´ m·ªçi tr√°ch nhi·ªám ph√°t sinh t·ª´ vi·ªác s·ª≠ d·ª•ng tr√°i quy ƒë·ªãnh; ng∆∞·ªùi d√πng t·ª± ch·ªãu tr√°ch nhi·ªám tr∆∞·ªõc PH√ÅP LU·∫¨T v·ªÅ h√†nh vi c·ªßa m√¨nh.{RESET}")

    if MAKE_COLOR:

        crypto_print(
            f"üïí Th·ªùi gian b·∫Øt ƒë·∫ßu:  {__import__('datetime').datetime.now().strftime('%H:%M:%S')}",
            GREEN,
        )

    start_time = __import__("time").time()

    if DOWNLOAD_IMAGE:
        os.makedirs(save_dir, exist_ok=True)
        consecutive_failures = 0

        for i in range(1, total_pages + 1):
            file_name = f"{i}.jpg"
            url = f"{base_url}/{file_name}?{id_sach}"

            try:
                res = requests.get(url, headers=headers)
                if res.status_code == 200:
                    with open(os.path.join(save_dir, file_name), "wb") as f:
                        f.write(res.content)
                    if DEBUG_PROGRAM:
                        print(f"{GREEN}‚úî Downloaded {file_name}{RESET}")
                    consecutive_failures = 0
                else:
                    consecutive_failures += 1
                    if DEBUG_PROGRAM:
                        print(f"{RED}‚ùå Failed (Status {res.status_code}): {file_name}{RESET}")

                    MAX_ERROR_ALLOWED = 5
                    if ENABLE_FAILED_IMAGE is False and consecutive_failures >= MAX_ERROR_ALLOWED:
                        if DEBUG_PROGRAM:
                            print(f"\n{YELLOW}‚ö†Ô∏è  Ph√°t hi·ªán {MAX_ERROR_ALLOWED} ·∫£nh l·ªói li√™n ti·∫øp{RESET}")
                            print(f"{YELLOW}‚ö†Ô∏è  ENABLE_FAILED_IMAGE = False ‚Üí D·ª´ng download ·∫£nh{RESET}")
                        break
            except Exception as e:
                consecutive_failures += 1
                if DEBUG_PROGRAM:
                    print(f"{RED}‚ùå Error downloading {file_name}: {e}{RESET}")
                if ENABLE_FAILED_IMAGE is False and consecutive_failures >= MAX_ERROR_ALLOWED:
                    if DEBUG_PROGRAM:
                        print(f"\n{YELLOW}‚ö†Ô∏è  Ph√°t hi·ªán {consecutive_failures} ·∫£nh l·ªói li√™n ti·∫øp{RESET}")
                        print(f"{YELLOW}‚ö†Ô∏è  ENABLE_FAILED_IMAGE = False ‚Üí D·ª´ng download ·∫£nh{RESET}")
                    break

        if DEBUG_PROGRAM:
            print(f"\n{CYAN}üéâ DONE ‚Äî All images saved in /{save_dir}{RESET}")

    if MAKE_COLOR:
        crypto_print("‚è≥ Processing...", YELLOW)

    if SHOULD_MERGE_TO_PDF:
        if DEBUG_PROGRAM:
            print(f"\n{YELLOW}‚è≥ Merging images into PDF...{RESET}")

        files = sorted(
            [f for f in os.listdir(save_dir) if f.endswith(".jpg")],
            key=lambda x: int(x.split(".")[0]),
        )

        if not files:
            print(f"{RED}‚ùå Kh√¥ng c√≥ file ·∫£nh n√†o ƒë·ªÉ merge{RESET}")
        else:
            images = [Image.open(os.path.join(save_dir, f)).convert("RGB") for f in files]
            images[0].save(output_pdf, save_all=True, append_images=images[1:])

            if MAKE_COLOR:
                crypto_print("‚úÖ Finishing...", GREEN)
            text = f"\n{GREEN}üìå PDF created successfully ‚Üí {output_pdf}{RESET}"

            for char in text:
                sys.stdout.write(char)
                sys.stdout.flush()
                time.sleep(0.05)

            if MAKE_COLOR:
                end_time = __import__("time").time()
                duration = end_time - start_time
                crypto_print(
                    f"üïí Th·ªùi gian k·∫øt th√∫c: {__import__('datetime').datetime.now().strftime('%H:%M:%S')}",
                    BLUE,
                    speed=0.02,
                    noise_level=10,
                )

                print()

                crypto_print(
                    f"‚è±Ô∏è  Th·ªùi gian x·ª≠ l√Ω: {duration:.2f} gi√¢y",
                    MAGENTA,
                    speed=0.02,
                    noise_level=10,
                )

                print("SUCCESSFULLY")

            if DEBUG_PROGRAM:
                print(f"{CYAN}üéâ DONE{RESET}")

            if DELETE_FOLDER_AFTER_MERGE:
                for f in files:
                    os.remove(os.path.join(save_dir, f))
                os.rmdir(save_dir)
                if DEBUG_PROGRAM:
                    print(f"{MAGENTA}üóë Temporary folder '{save_dir}' deleted{RESET}")

    cleanup_temp_files()


if __name__ == "__main__":
    main()
